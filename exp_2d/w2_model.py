import torch
import utils
import functools
import losses
import networks
import itertools
from torch.autograd import Variable
from collections import OrderedDict
from base_model import Base

class W2(Base):
    """Wasserstein-2 based models including W2-OT/W2GAN"""

    def get_data(self, config):
        """override z with gz in the case gen=T"""
        z = utils.to_var(next(self.z_generator))
        gz = self.g(z) if config.gen else z 
        # ZMALIK 20230718 Include sample generated by previous generator, if available
        if hasattr(self, 'g_min1'):
          g_min1_z = self.g_min1(z)
        else:
          g_min1_z = []
        r = utils.to_var(next(self.r_generator))
        return r, gz, g_min1_z, z

    def define_d(self, config):
        self.phi, self.eps = networks.get_d(config), networks.get_d(config)
        self.d_optimizer = networks.get_optim(itertools.chain(list(self.phi.parameters()),
                                                              list(self.eps.parameters())),
                                                              config.d_lr, config)

    def psi(self, y):
        return -self.phi(y) + self.eps(y)

    # 20210621 ZMALIK Commented out code to put it back in base_model
    #def get_dux(self, x, reverse=False):
    #    x = Variable(x.data, requires_grad=True)
    #    if reverse:
    #        ux = self.psi(x)
    #    else:
    #        ux = self.phi(x)
    #    dux = torch.autograd.grad(outputs=ux, inputs=x,
    #                              grad_outputs=utils.get_ones(ux.size()),
    #                              create_graph=True, retain_graph=True,
    #                              only_inputs=True)[0]
    #    return dux

    #def follow_ode(self, x, y, ux, vy, config):
    #    dvy = self.get_dux(y)
    #    delta_t = config.delta_t
    #    yn1 = y + delta_t*dvy
    #    return yn1

    def calc_dloss(self, x, y, tx, ty, ux, vy, config):
        d_loss = -torch.mean(ux + vy)
        if config.ineq:
            d_loss += losses.ineq_loss(x, y, ux, vy, self.cost, config.lambda_ineq)
        if config.ineq_interp:
            d_loss += losses.calc_interp_ineq(x, y, self.phi, self.psi, self.cost, config.lambda_ineq, losses.ineq_loss)
        if config.eq_phi:
            d_loss += losses.calc_eq(x, tx, self.phi, self.psi, self.cost, config.lambda_eq)
        if config.eq_psi:
            d_loss += losses.calc_eq(ty, y, self.phi, self.psi, self.cost, config.lambda_eq)
        return d_loss

    def calc_gloss(self, x, y, y1, ux, vy, config):
        """Computes generator loss by implicitly performing p.w. matching."""
        if config.follow_ode: # Explicitly follow ODE and do MSE fitting
          #yn1 = self.follow_ode(x, y, ux, vy, config) 20230616 -> Moved to outer loop
          if config.shuffle:
            y1 = utils.shuffle(y1, y)
          #yn1 = yn1.detach() -> Moved to outer loop
          gloss = torch.nn.MSELoss()
          return gloss(y,y1); 
        else:
          return torch.mean(vy) #Original update rule found in Leygonie et al

    def get_stats(self,  config):
        """print outs"""
        stats = OrderedDict()
        stats['loss/disc'] = self.d_loss
        if config.gen:
            stats['loss/gen'] = self.g_loss
        return stats

    def get_networks(self, config):
        nets = OrderedDict([('phi', self.phi),
                            ('eps', self.eps)])
        if config.gen:
            nets['gen'] = self.g
        return nets
    

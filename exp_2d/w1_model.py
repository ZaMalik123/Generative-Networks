import torch
import utils
import losses
import networks
from collections import OrderedDict
from torch.autograd import Variable
from base_model import Base

class W1(Base):
    """Wasserstein-1 based models including WGAN-GP/WGAN-LP"""

    def get_data(self, config):
        """override z with gz in the case gen=T"""
        z = utils.to_var(next(self.z_generator))
        gz = self.g(z) if config.gen else z
        # ZMALIK 20230721 Include sample generated by previous generator, if available -> Copied from w2_model
        if hasattr(self, 'g_min1'):
          g_min1_z = self.g_min1(z)
        else:
          g_min1_z = []
        r = utils.to_var(next(self.r_generator))
        return r, gz, g_min1_z, z

    def define_d(self, config):
        """Define discriminator (imported from configuration)"""
        self.phi = networks.get_d(config)
        self.d_optimizer = networks.get_optim(self.phi.parameters(), config.d_lr, config)

    def psi(self, y):
        return -self.phi(y)

    # 20230621 ZMALIK -> Commented code to put back in base model
    #def get_dux(self, x, reverse=False):
    #    """Differentiate the discriminator"""
    #    x = Variable(x.data, requires_grad=True)
    #    if reverse:
    #        ux = self.psi(x)
    #    else:
    #        ux = self.phi(x)
    #    dux = torch.autograd.grad(outputs=ux, inputs=x,
    #                              grad_outputs=utils.get_ones(ux.size()),
    #                              create_graph=True, retain_graph=True,
    #                              only_inputs=True)[0]
    #    return dux

    #def follow_ode(self, x, y, ux, vy):
    #    dvy = self.get_dux(y)
    #    yn1 = y + dvy
    #    return yn1

    def calc_dloss(self, x, y, tx, ty, ux, vy, config):
        d_loss = -torch.mean(ux + vy)
        d_loss += losses.gp_loss(x, y, self.phi, config.lambda_gp, clamp=config.clamp)
        return d_loss

    def calc_gloss(self, x, y, y1, ux, vy, config):
       if config.follow_ode: # Explicitly follow ODE and do MSE fitting
          #yn1 = self.follow_ode(x, y, ux, vy)
          if config.shuffle:
            yn1 = utils.shuffle(y1, y)
          #yn1 = yn1.detach()
          gloss = torch.nn.MSELoss()
          return gloss(y,y1); 
       else:
          return torch.mean(vy) #Original update rule found in Leygonie et al


    ## model statistics
    def get_stats(self,  config):
        """print outs"""
        stats = OrderedDict()
        stats['loss/disc'] = self.d_loss
        if config.gen:
            stats['loss/gen'] = self.g_loss
        return stats

    def get_networks(self, config):
        nets = OrderedDict([('phi', self.phi)])
        if config.gen:
            nets['gen'] = self.g
        return nets
